{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEOAzKAsOtP8nB9jrEHTSH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MestDash/PID/blob/main/notebooks/NN_optimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing some stuff"
      ],
      "metadata": {
        "id": "OVZzEVO3yL4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler, OneHotEncoder, RobustScaler, QuantileTransformer, PowerTransformer\n",
        "from sklearn.impute import KNNImputer, SimpleImputer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf, random\n",
        "from tensorflow.keras import layers, models, optimizers, regularizers, losses, metrics\n",
        "import os\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TerminateOnNaN\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import roc_curve\n",
        "import seaborn as sns\n",
        "import statistics\n",
        "import matplotlib.patches as patches\n",
        "from sklearn.metrics import balanced_accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "!pip install bayesian-optimization\n",
        "from bayes_opt import BayesianOptimization"
      ],
      "metadata": {
        "id": "wHZbOyIUQKxo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some functions"
      ],
      "metadata": {
        "id": "GPTs5EY8yy7N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VA_V1YZJTd8u"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='tensorflow')\n",
        "\n",
        "def find_optimal_threshold(y_true, y_prob, metric=\"balanced_accuracy\", num_thresholds=200):\n",
        "    \"\"\"\n",
        "    Find the threshold that maximizes a given metric.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : array-like\n",
        "        True binary labels (0/1).\n",
        "    y_prob : array-like\n",
        "        Predicted probabilities for the positive class.\n",
        "    metric : str\n",
        "        Metric to optimize. One of {\"balanced_accuracy\", \"f1\", \"precision\", \"recall\"}.\n",
        "    num_thresholds : int\n",
        "        Number of thresholds to evaluate between 0 and 1.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    best_thresh : float\n",
        "        Threshold that maximizes the chosen metric.\n",
        "    best_score : float\n",
        "        Score achieved at the best threshold.\n",
        "    \"\"\"\n",
        "    # Choose metric function\n",
        "    metric_funcs = {\n",
        "        \"balanced_accuracy\": balanced_accuracy_score,\n",
        "        \"f1\": f1_score,\n",
        "        \"precision\": precision_score,\n",
        "        \"recall\": recall_score,\n",
        "    }\n",
        "    if metric not in metric_funcs:\n",
        "        raise ValueError(f\"Unknown metric '{metric}'. Choose from {list(metric_funcs.keys())}\")\n",
        "\n",
        "    best_thresh, best_score = 0.5, -1\n",
        "    thresholds = np.linspace(0.0, 1.0, num_thresholds)\n",
        "\n",
        "    for t in thresholds:\n",
        "        preds = (y_prob >= t).astype(int)\n",
        "        try:\n",
        "            score = metric_funcs[metric](y_true, preds)\n",
        "        except ValueError:\n",
        "            # Can happen if no positive predictions\n",
        "            continue\n",
        "        if score > best_score:\n",
        "            best_thresh, best_score = t, score\n",
        "\n",
        "    #print(f\"[Threshold Optimization] Best {metric}: {best_score:.4f} at threshold={best_thresh:.3f}\")\n",
        "    return best_thresh\n",
        "\n",
        "def stratified_split(df: pd.DataFrame, test_size: float = 0.2, random_state: int = 17):\n",
        "    X = df.drop(columns=['IUIS', 'IUIS extended', 'PCODE','y'])\n",
        "    y = df['y']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=test_size,\n",
        "        stratify=y,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "def labelize(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    def assign_label(row):\n",
        "        # Handle both dash and no-dash variants\n",
        "        if row['IUIS'] in ['No arguments for lymphoid-PID', 'No arguments for lymphoid PID']:\n",
        "            return 'DC'\n",
        "        else:\n",
        "            main_label = row['IUIS'].split(':')[0].strip()\n",
        "            if main_label == 'III':\n",
        "                try:\n",
        "                    first_digit = int(str(row['IUIS extended'])[0])\n",
        "                    if first_digit == 4:\n",
        "                        return 'IIIb'\n",
        "                    else:\n",
        "                        return 'IIIa'\n",
        "                except (ValueError, IndexError):\n",
        "                    return 'IIIa'  # default if parsing fails\n",
        "            else:\n",
        "                return main_label\n",
        "\n",
        "    df['y'] = df.apply(assign_label, axis=1)\n",
        "    return df\n",
        "\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "N_SPLITS = 5\n",
        "CORR_THRESHOLD = 0.95\n",
        "N_EPOCHS_BIN = 50\n",
        "N_EPOCHS_MULTI = 50\n",
        "BATCH_SIZE = 32\n",
        "SMOTE_IMBALANCE_THRESHOLD = 0.5  # trigger SMOTE when minority/majority ratio < this\n",
        "\n",
        "np.random.seed(RANDOM_STATE)\n",
        "random.seed(RANDOM_STATE)\n",
        "tf.random.set_seed(RANDOM_STATE)\n",
        "\n",
        "# Target classes (canonical order)\n",
        "ALL_CLASSES = [\"DC\", \"I\", \"II\", \"IIIa\", \"IIIb\", \"IV\"]\n",
        "STAGE3_CLASSES = [\"II\", \"IIIa\", \"IIIb\", \"IV\"]  # in this order for softmax head\n",
        "\n",
        "class FeatureFilter(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, corr_threshold=0.95):\n",
        "        self.corr_threshold = corr_threshold\n",
        "        self.to_drop_ = []\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Keep only numeric columns\n",
        "        numeric_df = X.select_dtypes(include=[np.number])\n",
        "\n",
        "        # Drop constant features\n",
        "        constant_features = [col for col in numeric_df.columns if numeric_df[col].nunique() <= 1]\n",
        "\n",
        "        # Drop highly correlated features\n",
        "        corr_matrix = numeric_df.corr().abs()\n",
        "        upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "        high_corr = [column for column in upper_triangle.columns if any(upper_triangle[column] > self.corr_threshold)]\n",
        "\n",
        "        # Merge and store\n",
        "        self.to_drop_ = list(set(constant_features + high_corr))\n",
        "        #print('These features will be dropped:')\n",
        "        #print(self.to_drop_)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return X.drop(columns=self.to_drop_, errors='ignore')\n",
        "\n",
        "\n",
        "def build_preprocessor_binary(X_train: pd.DataFrame):\n",
        "    X_train = X_train.copy()\n",
        "    num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cat_cols = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "    cat_pipeline = Pipeline(\n",
        "        steps=[\n",
        "            (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"NA\")),\n",
        "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    numeric_pipe = Pipeline(steps=[\n",
        "        (\"scale\",     QuantileTransformer(n_quantiles=X_train.shape[0])),\n",
        "        (\"impute\", KNNImputer(n_neighbors=5)),\n",
        "        (\"minmax\",      MinMaxScaler())\n",
        "    ])\n",
        "\n",
        "    pre = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", numeric_pipe, num_cols),\n",
        "            (\"cat\", cat_pipeline, cat_cols)\n",
        "        ],\n",
        "        remainder=\"drop\"\n",
        "    )\n",
        "    pre.fit(X_train)\n",
        "    return pre\n",
        "\n",
        "\n",
        "def build_preprocessor_multi(X_train: pd.DataFrame):\n",
        "    X_train = X_train.copy()\n",
        "    num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cat_cols = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "    cat_pipeline = Pipeline(\n",
        "        steps=[\n",
        "            (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"NA\")),\n",
        "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    numeric_pipe = Pipeline(steps=[\n",
        "        (\"scale\",     PowerTransformer()),\n",
        "        (\"impute\", KNNImputer(n_neighbors=5))\n",
        "    ])\n",
        "\n",
        "    pre = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", numeric_pipe, num_cols),\n",
        "            (\"cat\", cat_pipeline, cat_cols)\n",
        "        ],\n",
        "        remainder=\"drop\"\n",
        "    )\n",
        "    pre.fit(X_train)\n",
        "    return pre"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_binary_nn(input_dim):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(128, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_multiclass_nn(input_dim, n_classes):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(128, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(n_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=Adam(1e-3),\n",
        "        loss='sparse_categorical_crossentropy',  # <-- fixed here\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "P8De6niL_SSN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_stage1_labels(y):\n",
        "    # I vs Rest\n",
        "    return (y == \"I\").astype(int)\n",
        "\n",
        "def build_stage2_labels(y):\n",
        "    # DC vs Rest (but we train Stage2 on the subset y != I)\n",
        "    mask = (y != \"I\")\n",
        "    y2 = (y[mask] == \"DC\").astype(int)\n",
        "    return mask, y2\n",
        "\n",
        "def build_stage3_labels(y):\n",
        "    # Multiclass on {II, IIIa, IIIb, IV}; we train Stage3 on y not in {I, DC}\n",
        "    mask = ~y.isin([\"I\", \"DC\"])\n",
        "    y3 = y[mask]\n",
        "    # Map to indices in STAGE3_CLASSES order\n",
        "    y3_idx = y3.map({c: i for i, c in enumerate(STAGE3_CLASSES)})\n",
        "    return mask, y3_idx\n",
        "\n",
        "def soft_gated_combine_probs(pI, pDC, pStage3_rows):\n",
        "    \"\"\"\n",
        "    pI:    (n,) probability of \"I\"\n",
        "    pDC:   (n,) probability of \"DC\"\n",
        "    pS3:   (n,4) probabilities over [II, IIIa, IIIb, IV]\n",
        "    Returns:\n",
        "      final_probs: (n, 6) in class order ALL_CLASSES\n",
        "    \"\"\"\n",
        "    n = len(pI)\n",
        "    final = np.zeros((n, 6), dtype=float)\n",
        "    # indices in ALL_CLASSES\n",
        "    idx = {c: i for i, c in enumerate(ALL_CLASSES)}\n",
        "    for i in range(n):\n",
        "        p_a = pI[i]\n",
        "        p_b = (1 - p_a) * pDC[i]\n",
        "        rest = (1 - p_a) * (1 - pDC[i]) * pStage3_rows[i]  # length 4\n",
        "        final[i, idx[\"I\"]] = p_a\n",
        "        final[i, idx[\"DC\"]] = p_b\n",
        "        # map stage3 probs to class indices\n",
        "        for j, cls in enumerate(STAGE3_CLASSES):\n",
        "            final[i, idx[cls]] = rest[j]\n",
        "        # normalize to sum 1 (numerical safety)\n",
        "        s = final[i].sum()\n",
        "        if s > 0:\n",
        "            final[i] /= s\n",
        "        else:\n",
        "            final[i, :] = 1.0 / 6.0\n",
        "    return final\n",
        "\n",
        "def maybe_smote(X_tr, y_tr, is_multiclass=False):\n",
        "    cnt = Counter(y_tr)\n",
        "    if len(cnt) <= 1:\n",
        "        return X_tr, y_tr  # nothing to do\n",
        "    minc, maxc = min(cnt.values()), max(cnt.values())\n",
        "    if (minc / maxc) < SMOTE_IMBALANCE_THRESHOLD:\n",
        "        if is_multiclass:\n",
        "            sm = SMOTE(random_state=RANDOM_STATE)\n",
        "        else:\n",
        "            sm = SMOTE(random_state=RANDOM_STATE)\n",
        "        X_tr, y_tr = sm.fit_resample(X_tr, y_tr)\n",
        "        print(\"SMOTE oversampling:\", Counter(y_tr))\n",
        "    return X_tr, y_tr\n"
      ],
      "metadata": {
        "id": "lAPSDIvWPVQe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization"
      ],
      "metadata": {
        "id": "-uw1odyJ_Bc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom transformer for conditional log1p\n",
        "class ConditionalLog1p(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.cols_to_transform = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = pd.DataFrame(X)\n",
        "        # store which columns need log1p (max > 1)\n",
        "        self.cols_to_transform = [\n",
        "            col for col in X.columns if X[col].max() > 1\n",
        "        ]\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = pd.DataFrame(X).copy()\n",
        "        for col in self.cols_to_transform:\n",
        "            X[col] = np.log1p(X[col])\n",
        "        return X.values\n",
        "\n",
        "# Preprocessing pipeline\n",
        "def build_preprocessor(X_train: pd.DataFrame, stage):\n",
        "    X_train = X_train.copy()\n",
        "    num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cat_cols = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "    cat_pipeline = Pipeline(\n",
        "        steps=[\n",
        "            (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"NA\")),\n",
        "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    if stage == 1:\n",
        "        numeric_pipe = Pipeline(steps=[\n",
        "            (\"scale\",     QuantileTransformer(n_quantiles=X_train.shape[0])),\n",
        "            (\"impute\", KNNImputer(n_neighbors=5)),\n",
        "            (\"minmax\",      MinMaxScaler())\n",
        "        ])\n",
        "\n",
        "    elif stage == 2:\n",
        "        numeric_pipe = Pipeline(steps=[\n",
        "            (\"scale\",     ConditionalLog1p()),\n",
        "            (\"impute\", KNNImputer(n_neighbors=5)),\n",
        "            (\"minmax\",      MinMaxScaler())\n",
        "        ])\n",
        "\n",
        "    elif stage == 3:\n",
        "        numeric_pipe = Pipeline(steps=[\n",
        "            (\"scale\",     PowerTransformer()),\n",
        "            (\"impute\", KNNImputer(n_neighbors=5))\n",
        "        ])\n",
        "\n",
        "    elif stage == 4:\n",
        "        numeric_pipe = Pipeline(steps=[\n",
        "            (\"scale\",     RobustScaler()),\n",
        "            (\"impute\", KNNImputer(n_neighbors=5)),\n",
        "            (\"minmax\",      MinMaxScaler())\n",
        "        ])\n",
        "\n",
        "    pre = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", numeric_pipe, num_cols),\n",
        "            (\"cat\", cat_pipeline, cat_cols)\n",
        "        ],\n",
        "        remainder=\"drop\"\n",
        "    )\n",
        "    pre.fit(X_train)\n",
        "    return pre\n",
        "\n",
        "# Preprocessing pipeline\n",
        "def build_preprocessor_multi(X_train: pd.DataFrame):\n",
        "    X_train = X_train.copy()\n",
        "    num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cat_cols = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "    cat_pipeline = Pipeline(\n",
        "        steps=[\n",
        "            (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"NA\")),\n",
        "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    numeric_pipe = Pipeline(steps=[\n",
        "        (\"scale\",     PowerTransformer()),\n",
        "        (\"impute\", KNNImputer(n_neighbors=5))\n",
        "    ])\n",
        "\n",
        "    pre = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", numeric_pipe, num_cols),\n",
        "            (\"cat\", cat_pipeline, cat_cols)\n",
        "        ],\n",
        "        remainder=\"drop\"\n",
        "    )\n",
        "    pre.fit(X_train)\n",
        "    return pre"
      ],
      "metadata": {
        "id": "qKl09JkO8a3Z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as K\n",
        "import gc\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=8,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.3,\n",
        "    patience=3,\n",
        "    min_lr=1e-7\n",
        ")\n",
        "\n",
        "callbacks = [early_stop, reduce_lr]\n",
        "\n",
        "def run_bayesopt_all(\n",
        "    X,\n",
        "    y,\n",
        "    stage=1,\n",
        "    label_builder=None,\n",
        "    mask=None,\n",
        "    init_points=25,\n",
        "    n_iter=50,\n",
        "    random_state=17,\n",
        "    corr_threshold=0.9,\n",
        "):\n",
        "    # --- Apply mask if provided ---\n",
        "    if mask is not None:\n",
        "        X = X.loc[mask]\n",
        "        y = y.loc[mask]\n",
        "\n",
        "    # --- Build labels (stage-specific if provided) ---\n",
        "    if label_builder is not None:\n",
        "        y_labels = label_builder(y)\n",
        "    else:\n",
        "        y_labels = y\n",
        "\n",
        "    # --- Encode labels ---\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(y_labels)\n",
        "    num_classes = len(le.classes_)\n",
        "    is_binary = num_classes == 2\n",
        "\n",
        "    # --- Categorical search spaces ---\n",
        "    activation_choices = [\"relu\", \"selu\", \"elu\"]\n",
        "    optimizer_choices = [\"adam\", \"sgd\", \"rmsprop\"]\n",
        "\n",
        "    # --- Model builder ---\n",
        "    def create_model(input_dim, stage, units, learning_rate, dropout_rate, num_hidden_layers, l2_reg, activation, optimizer_name):\n",
        "        model = Sequential()\n",
        "        model.add(Input(shape=(input_dim,)))\n",
        "\n",
        "        # Hidden layers\n",
        "        for _ in range(num_hidden_layers - 1):\n",
        "            model.add(Dense(units, activation=activation, kernel_regularizer=l2(l2_reg)))\n",
        "            if not stage == 1 or stage == 3:\n",
        "                model.add(BatchNormalization())\n",
        "            model.add(Dropout(dropout_rate))\n",
        "\n",
        "        if stage == 1 or stage == 2:\n",
        "            model.add(Dense(64, activation=activation, kernel_regularizer=l2(l2_reg)))\n",
        "            if stage == 1:\n",
        "                model.add(BatchNormalization())\n",
        "            model.add(Dropout(dropout_rate))\n",
        "            model.add(Dense(32, activation=activation, kernel_regularizer=l2(l2_reg)))\n",
        "            if stage == 1:\n",
        "                model.add(BatchNormalization())\n",
        "            model.add(Dropout(dropout_rate))\n",
        "        else:\n",
        "            model.add(Dense(128, activation=activation, kernel_regularizer=l2(l2_reg)))\n",
        "            if stage == 3:\n",
        "                model.add(BatchNormalization())\n",
        "            model.add(Dropout(dropout_rate))\n",
        "            model.add(Dense(64, activation=activation, kernel_regularizer=l2(l2_reg)))\n",
        "            if stage == 3:\n",
        "                model.add(BatchNormalization())\n",
        "            model.add(Dropout(dropout_rate))\n",
        "            model.add(Dense(32, activation=activation, kernel_regularizer=l2(l2_reg)))\n",
        "            if stage == 3:\n",
        "                model.add(BatchNormalization())\n",
        "            model.add(Dropout(dropout_rate))\n",
        "\n",
        "        # Output layer\n",
        "        if is_binary:\n",
        "            model.add(Dense(1, activation=\"sigmoid\", kernel_regularizer=l2(l2_reg)))\n",
        "            loss_fn = \"binary_crossentropy\"\n",
        "        else:\n",
        "            model.add(Dense(num_classes, activation=\"softmax\", kernel_regularizer=l2(l2_reg)))\n",
        "            loss_fn = \"sparse_categorical_crossentropy\"\n",
        "\n",
        "        # Select optimizer\n",
        "        if optimizer_name == \"adam\":\n",
        "            optimizer = Adam(learning_rate=learning_rate)\n",
        "        elif optimizer_name == \"sgd\":\n",
        "            optimizer = SGD(learning_rate=learning_rate, momentum=0.9)\n",
        "        elif optimizer_name == \"rmsprop\":\n",
        "            optimizer = RMSprop(learning_rate=learning_rate)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=loss_fn,\n",
        "            metrics=[\"accuracy\"],\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_and_evaluate(units, learning_rate, dropout_rate, num_hidden_layers,\n",
        "                       batch_size, epochs, l2_reg, activation_idx, optimizer_idx,\n",
        "                       threshold=0.5):\n",
        "        units = int(units)\n",
        "        num_hidden_layers = int(num_hidden_layers)\n",
        "        batch_size = int(batch_size)\n",
        "        epochs = int(epochs)\n",
        "        activation = activation_choices[int(round(activation_idx))]\n",
        "        optimizer_name = optimizer_choices[int(round(optimizer_idx))]\n",
        "\n",
        "        scores_upper = []\n",
        "\n",
        "        for i in range(5):\n",
        "            skf_outer = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
        "            scores = []\n",
        "\n",
        "            # Outer loop → test fold\n",
        "            for trainval_idx, test_idx in skf_outer.split(X, y_encoded):\n",
        "                X_trainval, X_test = X.iloc[trainval_idx], X.iloc[test_idx]\n",
        "                y_trainval, y_test = y_encoded[trainval_idx], y_encoded[test_idx]\n",
        "\n",
        "                # Inner split → train vs val (3 vs 1)\n",
        "                skf_inner = StratifiedKFold(n_splits=4, shuffle=True, random_state=random_state)\n",
        "                inner_train_idx, val_idx = next(skf_inner.split(X_trainval, y_trainval))\n",
        "\n",
        "                X_tr, X_val = X_trainval.iloc[inner_train_idx], X_trainval.iloc[val_idx]\n",
        "                y_tr, y_val = y_trainval[inner_train_idx], y_trainval[val_idx]\n",
        "\n",
        "                # --- Fit preprocessing only on training ---\n",
        "                ff = FeatureFilter(corr_threshold=corr_threshold)\n",
        "                ff.fit(X_tr)\n",
        "                X_tr = ff.transform(X_tr)\n",
        "                X_val = ff.transform(X_val)\n",
        "                X_test = ff.transform(X_test)\n",
        "\n",
        "                pre = build_preprocessor(X_tr, stage)\n",
        "\n",
        "                X_tr_prep = pre.fit_transform(X_tr)\n",
        "                X_val_prep = pre.transform(X_val)\n",
        "                X_test_prep = pre.transform(X_test)\n",
        "\n",
        "                input_dim = X_tr_prep.shape[1]\n",
        "\n",
        "                # --- Compute class weights ---\n",
        "                classes = np.unique(y_tr)\n",
        "                class_weights = compute_class_weight(\"balanced\", classes=classes, y=y_tr)\n",
        "                class_weight_dict = dict(zip(classes, class_weights))\n",
        "\n",
        "                # --- Build model ---\n",
        "                model = create_model(input_dim, stage, units, learning_rate, dropout_rate,\n",
        "                                 num_hidden_layers, l2_reg, activation, optimizer_name)\n",
        "\n",
        "                # --- Train model (monitor val fold) ---\n",
        "                model.fit(\n",
        "                    X_tr_prep,\n",
        "                    y_tr,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    verbose=0,\n",
        "                    class_weight=class_weight_dict,\n",
        "                    validation_data=(X_val_prep, y_val),\n",
        "                    callbacks=callbacks,\n",
        "                )\n",
        "\n",
        "                # --- Evaluate on test fold ---\n",
        "                y_pred_prob = model.predict(X_test_prep, verbose=0)\n",
        "                if is_binary:\n",
        "                    p_val = model.predict(X_val_prep, verbose=0).ravel()\n",
        "                    thresh = find_optimal_threshold(y_val, p_val)\n",
        "                    y_pred = (y_pred_prob.ravel() >= thresh).astype(int)\n",
        "                else:\n",
        "                    y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "                bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
        "                scores.append(bal_acc)\n",
        "\n",
        "            scores_upper.append(np.mean(scores))\n",
        "            del X_tr, X_val, X_test, X_tr_prep, X_val_prep, X_test_prep, model\n",
        "            K.clear_session()\n",
        "            gc.collect()\n",
        "\n",
        "        return np.mean(scores_upper)\n",
        "\n",
        "\n",
        "    # --- Define search space ---\n",
        "    pbounds = {\n",
        "        \"units\": (32, 256),\n",
        "        \"learning_rate\": (1e-4, 1e-2),\n",
        "        \"dropout_rate\": (0.0, 0.5),\n",
        "        \"num_hidden_layers\": (1, 5),\n",
        "        \"batch_size\": (16, 128),\n",
        "        \"epochs\": (30, 120),\n",
        "        \"l2_reg\": (1e-6, 1e-2),\n",
        "        \"activation_idx\": (0, len(activation_choices) - 1),\n",
        "        \"optimizer_idx\": (0, len(optimizer_choices) - 1),\n",
        "    }\n",
        "\n",
        "\n",
        "    optimizer = BayesianOptimization(\n",
        "        f=train_and_evaluate,\n",
        "        pbounds=pbounds,\n",
        "        random_state=random_state,\n",
        "        verbose=2,\n",
        "    )\n",
        "\n",
        "    optimizer.maximize(init_points=init_points, n_iter=n_iter)\n",
        "\n",
        "    return optimizer, le"
      ],
      "metadata": {
        "id": "NMl1kJFd1o9e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"df_cluster.csv\")\n",
        "df = labelize(df)\n",
        "X_train = df.drop(columns=['IUIS', 'IUIS extended', 'PCODE','y'])\n",
        "y_train = df['y']\n",
        "\n",
        "optimizer_stage1, le_stage1 = run_bayesopt_all(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    stage=1,\n",
        "    label_builder=build_stage1_labels,\n",
        "    init_points=15,\n",
        "    n_iter=50,\n",
        "    random_state=17,\n",
        "    corr_threshold=0.9\n",
        ")\n",
        "\n",
        "print(\"Best Stage 1 result:\", optimizer_stage1.max)\n",
        "print(\"Stage 1 classes:\", le_stage1.classes_)"
      ],
      "metadata": {
        "id": "vwolbOvfzEDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"df_cluster.csv\")\n",
        "df = labelize(df)\n",
        "X_train = df.drop(columns=['IUIS', 'IUIS extended', 'PCODE','y'])\n",
        "y_train = df['y']\n",
        "\n",
        "mask2, y2 = build_stage2_labels(y_train)\n",
        "\n",
        "optimizer_stage2, le_stage2 = run_bayesopt_all(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    stage=2,\n",
        "    label_builder=lambda yy: y2,\n",
        "    mask=mask2,\n",
        "    init_points=30,\n",
        "    n_iter=60,\n",
        "    corr_threshold=0.9\n",
        ")\n",
        "\n",
        "print(\"Best Stage 2 result:\", optimizer_stage2.max)\n",
        "print(\"Stage 2 classes:\", le_stage2.classes_)"
      ],
      "metadata": {
        "id": "bNcM7um4zORd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"df_cluster.csv\")\n",
        "df = labelize(df)\n",
        "X_train = df.drop(columns=['IUIS', 'IUIS extended', 'PCODE','y'])\n",
        "y_train = df['y']\n",
        "\n",
        "mask3, y3 = build_stage3_labels(y_train)\n",
        "\n",
        "optimizer_stage3, le_stage3 = run_bayesopt_all(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    stage=3,\n",
        "    label_builder=lambda yy: y3,\n",
        "    mask=mask1,\n",
        "    init_points=15,\n",
        "    n_iter=50,\n",
        "    corr_threshold=0.9\n",
        ")\n",
        "\n",
        "print(\"Best Stage 3 result:\", optimizer_stage3.max)\n",
        "print(\"Stage 3 classes:\", le_stage3.classes_)"
      ],
      "metadata": {
        "id": "vOOjhV1A8LGb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"df_cluster.csv\")\n",
        "df = labelize(df)\n",
        "X_train = df.drop(columns=['IUIS', 'IUIS extended', 'PCODE','y'])\n",
        "y_train = df['y']\n",
        "\n",
        "optimizer_all, le_ = run_bayesopt_all(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    stage=4,\n",
        "    label_builder=None,\n",
        "    init_points=30,\n",
        "    n_iter=60,\n",
        "    corr_threshold=0.9\n",
        ")\n",
        "\n",
        "print(\"Best result:\", optimizer_all.max)\n",
        "print(\"Classes:\", le_.classes_)"
      ],
      "metadata": {
        "id": "wXwTf7eT-yu5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}