# Extending Computational Cytometry Pipeline to Diagnose Primary Immunodeficiencies

The primary immunodeficiency diseases (PIDs) include many genetic disorders that affect different components of the innate and adaptive responses. The number of distinct genetic PIDs has increased exponentially with improved methods of detection and advanced laboratory methodology. Patients with PIDs have an increased susceptibility to infectious diseases and non-infectious complications including allergies, malignancies and autoimmune diseases (ADs). PIDs are characterized by heterogeneous clinical presentations, complex pathophysiology and multifactorial etiology.

Patients with primary immunodeficiencies (PID) have a defect in one or multiple elements of their immune system and show a wide spectrum of clinical manifestations. This heterogeneity often complicates a fast diagnosis which is necessary for a timely treatment and the prevention of potential life-threatening diseases. To cope with these issues, flow cytometry functions as an important tool in the diagnostic and prognostic work-up of PID. However, data analysis of cytometry data remains challenging.

To be able to immunophenotypically distinguish and diagnose PID subtypes, a computational pipeline on historical data from the Ghent University Hospital was built. Although the existing pipeline seems to classify the subtypes well, the limitation of using data from one research centre highlights the need for improvement and generalization in order to be used in other hospitals. The aim of this project is to improve the already existing pipeline, focusing on trying different normalization techniques over the available datasets and supervised approach. Improving and validating the model over datasets originated from different hospitals may give valuable insights on the model usage in different clinical settings.

<img width="1324" height="1093" alt="hierarchical_diagram" src="https://github.com/user-attachments/assets/30db71e3-4c95-4c57-8693-da720f05ff80" />

Fifteen preprocessing configurations combining different scaling and normalization methods were compared across the three hierarchical stages and a flat multiclass classifier. Results revealed that preprocessing choices substantially influenced performance, with differences exceeding 10% in balanced accuracy between the best and worst configurations. No single approach performed optimally across all stages. Skewness-aware transformations, such as the QuantileTransformer, PowerTransformer, and conditional logarithmic scaling, consistently outperformed simple normalization methods in most cases. However, the StandardScaler and RobustScaler provided competitive results, particularly for intermediate or multiclass stages. The best-performing configurations were subsequently integrated into a unified, task-specific preprocessing pipeline.

Following preprocessing selection, Bayesian optimization was applied to refine the neural network architectures. For stages 1 and 2, optimization confirmed that simple architectures without hidden layers were sufficient, provided appropriate regularization and activation functions were used. Both stages favoured the SeLU activation and moderate dropout (≈0.35), while SGD and RMSProp were alternately selected as optimal optimizers depending on the task. In contrast, stage 3 benefited from increased depth, with two additional hidden layers of 86 neurons. The flat classifier required no additional layers, instead achieving performance gains through optimized regularization and learning rate scheduling. These findings collectively suggest that architectural simplicity can be effective in early hierarchical stages, while greater depth becomes beneficial only in more complex contexts.

Using the optimal preprocessing and architectures, 5-fold cross-validation demonstrated consistent improvements across all hierarchical stages relative to the baseline. Stage 1 achieved the highest performance (balanced accuracy = 0.899 ± 0.013), reliably identifying IUIS class I samples with near-perfect recall. Stage 2 yielded a balanced accuracy of 0.838 ± 0.002, showing strong sensitivity for diseased controls but a tendency toward false positives. The multiclass stage (stage 3) showed the largest improvement relative to baseline, achieving a balanced accuracy of 0.530 ± 0.012. When integrated into the soft-gated hierarchical model, the overall balanced accuracy reached 0.517 ± 0.012. This configuration demonstrated the ability to correct local misclassifications through probabilistic integration, occasionally recovering cases missed at earlier stages. The flat multiclass classifier performed comparably (0.540 ± 0.011), showing stronger average class balance but weaker precision for specific minority classes.

<img width="1718" height="1137" alt="CV_diagram" src="https://github.com/user-attachments/assets/3498c148-fd98-4422-a230-59069b75097e" />

When evaluated on unseen data, performance decreased moderately, reflecting expected generalization gaps. Stage 1 maintained robust performance (balanced accuracy = 0.919 ± 0.011) with good recall for IUIS class I (0.943), confirming its strong generalization and appropriate sensitivity-precision trade-off. Stage 2 exhibited a larger performance drop (0.599 ± 0.022), characterized by asymmetric recall–precision trade-offs between the DC and PID classes. Stage 3 also experienced a pronounced degradation (0.335 ± 0.026), performing reliably only on the dominant IUIS class III.4 while struggling with minority categories (classes II and IV). When operating as an ensemble with the soft-gated approach, the hierarchical classifier achieved a balanced accuracy of 0.354 ± 0.014. The flat classifier, which also registered a substantial drop in performance compared to CV, achieved a balanced accuracy of 0.373 ± 0.009. In both cases, macro-average metrics confirmed imbalanced per-class performance, with stronger results for the most represented disease categories and poor recognition of rare classes, although sensitivity for IUIS class I samples was consistently high.

<img width="1189" height="790" alt="comparison_performances" src="https://github.com/user-attachments/assets/5b0a2d4f-6d16-4232-97b1-7ed0629e5582" />
